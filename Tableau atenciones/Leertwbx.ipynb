{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tableauhyperapi\n",
      "  Using cached tableauhyperapi-0.0.19484-py3-none-win_amd64.whl.metadata (1.3 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting cffi!=1.14.3,<2,>=1.12.2 (from tableauhyperapi)\n",
      "  Using cached cffi-1.16.0-cp39-cp39-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Using cached numpy-2.0.0-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\documents\\github\\hackaton\\tableau atenciones\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pycparser (from cffi!=1.14.3,<2,>=1.12.2->tableauhyperapi)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\documents\\github\\hackaton\\tableau atenciones\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached tableauhyperapi-0.0.19484-py3-none-win_amd64.whl (53.8 MB)\n",
      "Using cached pandas-2.2.2-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Using cached cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Using cached numpy-2.0.0-cp39-cp39-win_amd64.whl (16.5 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pytz, tzdata, pycparser, numpy, pandas, cffi, tableauhyperapi\n",
      "Successfully installed cffi-1.16.0 numpy-2.0.0 pandas-2.2.2 pycparser-2.22 pytz-2024.1 tableauhyperapi-0.0.19484 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tableauhyperapi pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tableauhyperapi import HyperProcess, Connection, Telemetry, TableName, Inserter, HyperException\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twbx_url = 'https://public.tableau.com/workbooks/Atenciones_HisMinsa_17005837025700.twb'\n",
    "twbx_file_path = 'Atenciones_HisMinsa.twbx'\n",
    "extracted_folder = 'folder_extraido'\n",
    "output_csv_path = 'tengohambre.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el archivo .twbx\n",
    "response = requests.get(twbx_url)\n",
    "with open(twbx_file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print(f\"Archivo descargado: {twbx_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomprimir el archivo .twbx\n",
    "with zipfile.ZipFile(twbx_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_extraido\\Data\\TableauTemp\\TEMP_0ifl42b0fpp8d811cnnoi11ygtcj.hyper\n"
     ]
    }
   ],
   "source": [
    "# Buscar archivos .hyper o .tde dentro de la carpeta extraída\n",
    "data_file_path = None\n",
    "for root, dirs, files in os.walk(extracted_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.hyper') or file.endswith('.tde'):\n",
    "            data_file_path = os.path.join(root, file)\n",
    "            break\n",
    "\n",
    "print(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas: 2188790\n",
      "Cantidad de columnas: 24\n",
      "Tamaño del resultado: 63124.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Lista de columnas que queremos conservar\n",
    "desired_columns = [\"ambito\", \"diris\", \"aniomes\", \"etapa\", \"id_genero\", \"eess_n\", \"eess_c\", \"eess_r\", \"fec_aten\", \"diriss\", \"Departamento\", \"Provincia\", \"Distrito\", \"REGION\", \"AREA_Res\", \"CCDD\", \"NOMBDEP\", \"CCPP\", \"NOMBPROV\", \"Area\", \"Length\"]\n",
    "\n",
    "# Información extraída del log\n",
    "num_rows = 2188790  # Número de filas\n",
    "num_cols = 24  # Número de columnas\n",
    "result_size_mb = 63124.1  # Tamaño del resultado en MB\n",
    "\n",
    "# Imprimir información del conjunto de datos\n",
    "print(f\"Cantidad de filas: {num_rows}\")\n",
    "print(f\"Cantidad de columnas: {num_cols}\")\n",
    "print(f\"Tamaño del resultado: {result_size_mb} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas deseadas para la salida\n",
    "desired_columns = [\n",
    "    \"ambito\", \"diris\", \"aniomes\", \"etapa\", \"id_genero\", \"eess_n\", \"eess_c\", \"eess_r\", \n",
    "    \"fec_aten\", \"diriss\", \"Departamento\", \"Provincia\", \"Distrito\", \"REGION\", \"AREA_Res\", \n",
    "    \"CCDD\", \"NOMBDEP\", \"CCPP\", \"NOMBPROV\", \"Area\", \"Length\"\n",
    "]\n",
    "\n",
    "# Definir filtros para la consulta\n",
    "year_filter = \"2023\"\n",
    "etapa_filter = \"01\"  # Ejemplo: \"< 01 mes\" debería ser ajustado según el formato de los datos\n",
    "region_filter = \"AREQUIPA\"\n",
    "genero_filter = \"F\"\n",
    "\n",
    "# Construir la consulta SQL con los filtros necesarios\n",
    "columns_str = ', '.join(desired_columns)\n",
    "query = f\"\"\"\n",
    "SELECT {columns_str} \n",
    "FROM \"Extract\".\"Extract\" \n",
    "WHERE aniomes LIKE '{year_filter}%' \n",
    "AND etapa = '{etapa_filter}' \n",
    "AND Departamento = '{region_filter}'\n",
    "AND id_genero = '{genero_filter}'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas deseadas (excluyendo Geometry, Geometry1 y Geometry2)\n",
    "desired_columns = [\n",
    "    \"ambito\", \"diris\", \"aniomes\", \"etapa\", \"id_genero\", \"eess_n\", \"eess_c\", \"eess_r\", \n",
    "    \"fec_aten\", \"diriss\", \"Departamento\", \"Provincia\", \"Distrito\", \"REGION\", \"AREA_Res\", \n",
    "    \"CCDD\", \"NOMBDEP\", \"CCPP\", \"NOMBPROV\", \"Area\", \"Length\"\n",
    "]\n",
    "\n",
    "# Definir filtros para la consulta\n",
    "year_filter = \"2023\"\n",
    "etapa_filter = \"0\"  # Ejemplo de etapa\n",
    "region_filter = \"AREQUIPA\"\n",
    "genero_filter = \"F\"\n",
    "\n",
    "# Construir la consulta SQL con los filtros necesarios\n",
    "columns_str = ', '.join(desired_columns)\n",
    "query = f\"\"\"\n",
    "SELECT {columns_str} \n",
    "FROM \"Extract\".\"Extract\" \n",
    "WHERE aniomes LIKE '{year_filter}%' \n",
    "AND etapa = '{etapa_filter}' \n",
    "AND Departamento = '{region_filter}'\n",
    "AND id_genero = '{genero_filter}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM \"Extract\".\"Extract\" \n",
    "WHERE aniomes LIKE '2023%' \n",
    "AND etapa = '0' \n",
    "AND id_genero = 'F'\n",
    "AND diris = 'AREQUIPA'\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT etapa\n",
    "FROM \"Extract\".\"Extract\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas en el esquema '\"public\"': []\n",
      "Tablas en el esquema '\"Extract\"': [TableName('Extract', 'Extract')]\n",
      "Datos guardados en archivo_salida.csv\n"
     ]
    }
   ],
   "source": [
    "# Conectar al archivo .hyper y listar las tablas\n",
    "with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "    with Connection(endpoint=hyper.endpoint, database=data_file_path) as connection:\n",
    "        # Listar las tablas disponibles en el archivo .hyper\n",
    "        catalog = connection.catalog\n",
    "        schema_names = catalog.get_schema_names()\n",
    "        \n",
    "        for schema in schema_names:\n",
    "            table_names = catalog.get_table_names(schema=schema)\n",
    "            print(f\"Tablas en el esquema '{schema}': {table_names}\")\n",
    "\n",
    "        # Acceder a la tabla identificada\n",
    "        table_name = TableName('Extract', 'Extract')  # Usar el esquema y nombre de tabla correctos\n",
    "        if table_name in table_names:\n",
    "            # Crear una lista para almacenar las filas\n",
    "            df_list = []\n",
    "\n",
    "            try:\n",
    "                # Iterar sobre las filas del resultado y agregarlas a la lista\n",
    "                with connection.execute_query(query=query) as result:\n",
    "                    for row in result:\n",
    "                        row_data = {desired_columns[i]: row[i] for i in range(len(desired_columns))}\n",
    "                        df_list.append(row_data)\n",
    "            except HyperException as e:\n",
    "                print(f\"Error al ejecutar la consulta: {e}\")\n",
    "\n",
    "            # Convertir la lista de filas a un DataFrame\n",
    "            df = pd.DataFrame(df_list, columns=desired_columns)\n",
    "\n",
    "            # Guardar el DataFrame en un archivo CSV\n",
    "            df.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Datos guardados en {output_csv_path}\")\n",
    "        else:\n",
    "            print(f\"La tabla '{table_name}' no existe en el archivo .hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas deseadas (excluyendo Geometry, Geometry1 y Geometry2)\n",
    "desired_columns = [\n",
    "    \"ambito\", \"diris\", \"aniomes\", \"etapa\", \"id_genero\", \"eess_n\", \"eess_c\", \"eess_r\", \n",
    "    \"fec_aten\", \"diriss\", \"Departamento\", \"Provincia\", \"Distrito\", \"REGION\", \"AREA_Res\", \n",
    "    \"CCDD\", \"NOMBDEP\", \"CCPP\", \"NOMBPROV\", \"Area\", \"Length\"\n",
    "]\n",
    "\n",
    "# Definir filtros para la consulta\n",
    "year_filter = \"2023\"\n",
    "etapa_filter = \"0\"  # Índice de etapa para '< 01 mes'\n",
    "genero_filter = \"F\"\n",
    "region_filter = \"AREQUIPA\"\n",
    "\n",
    "# Construir la consulta SQL\n",
    "query = f\"\"\"\n",
    "SELECT * \n",
    "FROM \"Extract\".\"Extract\" \n",
    "WHERE aniomes LIKE '{year_filter}%' \n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
